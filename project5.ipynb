{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1488c85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Project 5: Goals and Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d8c13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The goals of this assignment are:\n",
    "* To work with the object oriented version of our corpus code.\n",
    "* To modify a web app that we can use to analyze text data.\n",
    "* To finetune a transformer model and write a model card for it.\n",
    "\n",
    "Here are the steps you should do to successfully complete this project:\n",
    "1. Check out the assignment from Github. \n",
    "2. Make a codespace with at least 8GB of RAM (preferably more!).\n",
    "3. Copy your `spacy_on_corpus.py` from project 4b.\n",
    "4. Copy the anvil callable functions and your API key from project 4b into the file `server.py`.\n",
    "5. Complete all the instructions in this notebook. Make sure to answer all questions, and to commit the notebook in a \"run\" state!\n",
    "6. Edit the README.md file. Provide your name, your class year, links to/descriptions of any extensions and a list of resources you used. \n",
    "7. Commit your code often. We will take the last commit before the deadline as your submission of the project.\n",
    "\n",
    "Possible extensions (from least points to most points):\n",
    "* Modify the `render_document_sentiment` method you implemented for this project to have a third column, `Aspect`. Fill it with the first keyphrase extracted from the sentence using the keyphrase extraction algorithm from project 4b, or with the first noun chunk in the sentence. Explain whether this is better than the baseline implementation for this project, and why.\n",
    "* Finetune a different model (other than distilbert-cased) for the sentence sentiment task.\n",
    "* Finetune a transformer model for a different NLP task. Add it to your web app.\n",
    "* Your other ideas are welcome! If you'd like to discuss one with Dr Stent, feel free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Copy your `spacy_on_corpus.py` from project 4b.\n",
    "2. Copy the anvil callable functions from project 4b into the file `server.py`.\n",
    "3. Run % `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sure We Can Work With .py Files We Are Editing\n",
    "\n",
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a Corpus\n",
    "\n",
    "In the code cell below, build a corpus using `creator.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Server\n",
    "\n",
    "There are now two ways to start our server:\n",
    "\n",
    "1. From a notebook: import `server`, then call the `run` function.\n",
    "2. On the terminal: % `python server`.\n",
    "\n",
    "In the code cell below, try the first way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above code cells don't work, then you haven't followed the set up instructions. Go back to that section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a Transformer Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can currently get document-level sentiment but quite often a movie review is nuanced: some sentences say good things about some aspects of the movie, while others say bad things about other aspects. In this project, we will finetune a transformer model on the sentiment of sentences from movie reviews, and then add to our webapp the ability to see sentence-level sentiment for a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Some Labeled Data\n",
    "\n",
    "We are going to use the (SST)[https://huggingface.co/datasets/sst] dataset. Note the datasheet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets is a huggingface python package that makes it easy to download huggingface datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# download the sst dataset\n",
    "raw_sst = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# make it smaller for testing; once everything is working, train on all the data by commenting this line out and rerunning the notebook\n",
    "raw_sst = raw_sst.filter(lambda e, i: i<100, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we look at the dataset.\n",
    "\n",
    "**ALWAYS LOOK AT YOUR DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the sst dataset\n",
    "\n",
    "# look at the sst training data\n",
    "\n",
    "# look at the sst training data sentences. Note each data point is a pre-tokenized sentence.\n",
    "\n",
    "# look at the sst training data labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "1. *For supervised machine learning, each data point has to have what?*\n",
    "2. *Why do we split data for supervised machine learning into train, dev (validation) and test?*\n",
    "3. *How many datapoints are in the dataset altogether (train, validation and test)?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data\n",
    "\n",
    "We will use the small `distilbert` model for this project. So we want to use its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tokenize our data using that tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_tokenize(example):\n",
    "    \"\"\"Tokenizes the input data using the designated transformer tokenizer.\n",
    "\n",
    "    :param example: a text\n",
    "    :type example: str\n",
    "    \"\"\"\n",
    "    return tokenizer(example['sentence'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# this tokenizes the train, validation and test sets\n",
    "tokenized_sst = raw_sst.map(transformer_tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "4. *When we looked at our data, we saw that it was tokenized (kind of like spaCy tokenizes). Why do we need to tokenize again with the transformer tokenizer?*\n",
    "5. *Recall that a transformer has fixed width input. Look at the tokenize function above.*\n",
    "   * *If the input text is shorter, what does the toknizer do?*\n",
    "   * *If the input text is longer, what does the tokenizer do?*\n",
    "6. *What type of supervised machine learning could we do if our labels are numeric?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a Transformer Model\n",
    "\n",
    "We will use the distilbert model for efficiency.\n",
    "\n",
    "*Our classification task has how many labels?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Run a Trainer\n",
    "\n",
    "Huggingface gives us a nice clean way to train: the `Trainer`. Each trainer has training arguments - where you can set hyperparameters. We will make a default set of training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"sst_model\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add an accuracy metric from the `evaluate` package so we can see accuracy while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this huggingf\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Setup evaluation \n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute the metric!\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will make a trainer using the model, the training arguments, our train and our dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_sst[\"train\"], eval_dataset=tokenized_sst[\"validation\"], compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we train.\n",
    "\n",
    "**This step takes a long time.** If you want to speed it up, you will need a GPU! But codespaces don't currently have GPU options. So:\n",
    "\n",
    "1. Download this notebook.\n",
    "2. Open [https://colab.research.google.com](https://colab.research.google.com).\n",
    "3. Upload the notebook.\n",
    "4. Add a cell at the top of the notebook and in it type:\n",
    "```\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install evaluate\n",
    "```\n",
    "4. In the Runtime menu, choose `GPU`.\n",
    "5. Run the notebook there to train a model.\n",
    "6. Download the trained model.\n",
    "   * in a code cell, type `!tar -czf model.tgz sst-model`\n",
    "   * download model.tgz\n",
    "7. Upload the trained model here in the codespace.\n",
    "   * upload model.tgz\n",
    "   * in the terminal, type `!tar -xzf model.tgz`\n",
    "\n",
    "After this course, you can always still use codespaces, and I recommend it because of the tight integration with Github (so your code is saved!). You can *also* always use Colab. If you use Colab, your notebooks will be backed up in your Google Drive, but *no other files that you generated in colab are saved*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"sst-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "Now we should evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentence_sentiment = pipeline(\"text-classification\", model=\"sst-model\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try the model on a couple of sample sentences to sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_sentiment(\"This movie was awful!\"))\n",
    "\n",
    "print(sentence_sentiment(\"This movie was great!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to run the model on each of our test data points.\n",
    "\n",
    "Notice that the model outputs 'LABEL_0' or 'LABEL_1' while the data has labels -1 and 0. So *you* should define a function to map the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(output):\n",
    "    \"\"\"Gets a numeric label for the output from the classifier.\n",
    "        Sample output from classifier:  [{'label': 'LABEL_0', 'score': 0.61}] corresponds to label -1\n",
    "        Sample output from classifier: [{'label': 'LABEL_1', 'score': 0.72}] corresponds to label 0   \n",
    "\n",
    "        :param output: the output from the classifier\n",
    "        :type output: list[dict]\n",
    "        :returns: a label\n",
    "        :rtype: int\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the classifier on each dev data point.\n",
    "\n",
    "Each element in `raw_sst['validation']` is a dictionary with keys `idx`, `sentence` and `label`. For each dev data point, you should make a new dictionary with keys `idx`, `sentence`, `label` and `pred` (for the output from the classifier). Add this dictionary to the list of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# your work here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have:\n",
    "\n",
    "* `gold` labels, and\n",
    "* model predictions\n",
    "\n",
    "for each of the dev data points.\n",
    "\n",
    "We will calculate two metrics:\n",
    "\n",
    "1. accuracy\n",
    "2. confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "In the code cell below, implement the `accuracy` function. The accuracy of a classifier is the number of correctly labeled data points divided by the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(results):\n",
    "    \"\"\" Returns the accuracy of a list of classifier results\n",
    "\n",
    "    :param results: a list of dictionaries. Each dictionary contains, at minimum, the keys 'label' and 'pred'\n",
    "    :type results: list[dict]\n",
    "    :returns: accuracy\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the accuracy of the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "7. *How accurate is the finetuned model?*\n",
    "8. *What would be the accuracy of a simple model that flipped a coin?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "In the code cell below, implement the `confusion_matrix` function. A confusion matrix for a classifier is like a spreadsheet or table that has all the labels along the rows and columns. Each cell contains the number of data points where the gold label corresponded to that row label, and the predicted label to that column label.\n",
    "\n",
    "For example, for labels `TRUE` and `FALSE`, here is a possible confusion matrix:\n",
    "\n",
    "| | TRUE | FALSE |\n",
    "| --- | ---- | ----- |\n",
    "| TRUE | 5 | 2   |\n",
    "| FALSE | 1 | 4   | \n",
    "\n",
    "This says that there were 7 total data points with gold label `TRUE`, of which 5 had predicted label `TRUE`. There were 5 total data points with gold label `FALSE`, of which 4 had predicted label `FALSE`. This is a pretty good classifier!\n",
    "\n",
    "Your confusion matrix will be a dictionary of dictionaries. Here's the above confusion matrix as a dictionary of dictionaries:\n",
    "```\n",
    "cf = {'TRUE': {'TRUE': 5, 'FALSE': 2}, 'FALSE': {'TRUE': 1, 'FALSE': 4}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(results):\n",
    "    \"\"\" Returns the confusion matrix for a list of classifier results\n",
    "\n",
    "    :param results: a list of dictionaries. Each dictionary contains, at minimum, the keys 'label' and 'pred'\n",
    "    :type results: list[dict]\n",
    "    :returns: confusion matrix\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the confusion matrix for the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you become an advanced transformers tool builder, you could instead use the `evaluate` package made by the huggingface team.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "7. *For which class (-1 or 1, negative or positive sentiment) is the finetuned model most accurate?*\n",
    "8. *Why do you say that?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Your Finetuned Model to Your Webapp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Create a class attribute in the `corpus` class for loading your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create an instance method, `get_sentence_level_sentiment`, in the `corpus` class. This method should return a list of pairs `(sentence, label)` where `label` \n",
    " is the sentiment label for the sentence. Test it in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create an instance method, `render_document_sentiments`, in the `corpus` class. This method should return a markdown table for the document containing the sentences and their corresponding sentiment labels. At the bottom, it should have an extra row where the \"sentence\" is the string \"document\" and the label is the document-level sentiment (*not* from your finetuned model; from project 4b). Test it in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In `server.py`, add an anvil callable function, `get_doc_sentiment_markdown`, that calls `render_document_sentiments`. Test it in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Add a radio button 'Sentiment' to the `Analyze Document` form in your web app; when clicked, this should call `get_document_sentiment_markdown`. Paste a screenshot here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Card\n",
    "\n",
    "***ALWAYS DOCUMENT YOUR MODEL***\n",
    "\n",
    "Complete the model card reading in Perusall.\n",
    "\n",
    "Then, complete the model card in `model_card.md` for your finetuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* https://livingdatalab.com/posts/2023-04-23-fine-tuning-a-sentiment-analysis-model-with-huggingface.html\n",
    "* https://huggingface.co/docs/transformers/v4.15.0/model_sharing\n",
    "* https://huggingface.co/docs/datasets/process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
